{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download: 16S rRNA sequences\n",
    "\n",
    "Since 16S shows more promising results of identifying phylogeny of bacterial taxa, I will download a new set, but specifically look at entries under \"popset\" first. If there aren't enough sequences, I will expand the scope to include more sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "We will utilize the package [`BioPython`](https://biopython.org/wiki/Documentation) {cite}`Cock_2009` to interface with sequencial data hosted on NCBI. We will also use `pandas` {cite}`pandasdevelopmentteam2023, McKinney2010` to store metadata in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.SeqFeature import SeqFeature, Reference\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq, UndefinedSequenceError\n",
    "from Bio.Entrez.Parser import DictionaryElement\n",
    "from Bio.Entrez import HTTPError\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame, notna, merge, concat\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = \"kaedeito@student.ubc.ca\"\n",
    "# This line sets the name of the tool that is making the queries\n",
    "Entrez.tool = \"download_16s.ipynb\"\n",
    "\n",
    "SEARCH_NEW = False\n",
    "GRANULARITY = 5\n",
    "logger = logging.getLogger(\"download_16s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder to save our fasta and genbank file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath(\"..\\\\..\\\\datasets\\\\full_analysis\")\n",
    "os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and wrangle tables\n",
    "\n",
    "We need to clean the data to make it easier to work with.\n",
    "We need to load the metadata into a tabular format to make it easier to find problems, or to find interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table(df: DataFrame, granularity:int):\n",
    "  \"\"\"\n",
    "  Clean the dataframe by removing rows and cleaning string formatting.\n",
    "\n",
    "  :param `df`: The dataframe to clean.\n",
    "  :param `granularity`: The number of characters to group the ids by.\n",
    "  \"\"\"\n",
    "  feat_df = df.copy()\n",
    "\n",
    "  # Split the country column into two columns\n",
    "  feat_df[['country','iso_source2']] = feat_df['country'].str.split(':',expand=True)\n",
    "  # Trim whitespace from the isolation_source column\n",
    "  feat_df['iso_source2'] = feat_df['iso_source2'].map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "  # Group the ids by the first n characters\n",
    "  feat_df['group'] = feat_df['id'].str[:granularity]\n",
    "\n",
    "  # Fill the isolation_source column with the iso_source2 column if it is null\n",
    "  feat_df['isolation_source'] = feat_df['isolation_source'].fillna(feat_df['iso_source2'])\n",
    "\n",
    "  feat_df.loc[feat_df['group'] == 'KC668', 'country'] = 'Saudi Arabia'\n",
    "  feat_df.loc[feat_df['group'] == 'KC668', 'isolation_source'] = 'Southern Red Sea'\n",
    "\n",
    "  # Clean the table by removing the rows that do not contain Endozoicomonas\n",
    "  feat_df = feat_df[notna(feat_df['organism']) & (feat_df['organism'].astype(str).str.contains('Endozoicomonas') | feat_df['organism'].astype(str).str.contains('uncultured bacterium'))]\n",
    "\n",
    "  # Clean the table by removing the rows that do not contain 16S\n",
    "  feat_df = feat_df[notna(feat_df['product']) & (feat_df['product'].astype(str).str.contains('16S'))]\n",
    "\n",
    "  # Copy the colony info from the isolation_source column\n",
    "  if 'colony' in feat_df.columns:\n",
    "    feat_df['strain'] = feat_df['strain'].fillna(feat_df['colony'])\n",
    "\n",
    "  if 'BioProject' not in feat_df.columns:\n",
    "    feat_df['BioProject'] = np.nan\n",
    "\n",
    "  # Split the db_xref column into multiple columns, and use the first column as column name\n",
    "  feat_df_nuc_xref = feat_df[['id', 'db_xref']].copy()\n",
    "  feat_df_nuc_xref[['xref','value']] = feat_df['db_xref'].str.split(':',expand=True)\n",
    "\n",
    "  feat_df_nuc_xref = feat_df_nuc_xref.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "  feat_df_nuc_xref = feat_df_nuc_xref.pivot(index=['id'], columns='xref', values='value')\n",
    "  feat_df = merge(feat_df, feat_df_nuc_xref, on=\"id\")\n",
    "\n",
    "  # replace every \"Ken Ting\" with \"Kenting\"\n",
    "  feat_df['isolation_source'].replace('Ken Ting', 'Kenting', inplace=True)\n",
    "\n",
    "  # remove the columns that are not needed\n",
    "  feat_df.drop(columns=['db_xref'], inplace=True)\n",
    "\n",
    "  # Reset the index\n",
    "  feat_df.reset_index()\n",
    "\n",
    "  return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(to_combine: list[list[SeqRecord]], all_df: DataFrame):\n",
    "    idlist = all_df['id'].to_list()\n",
    "    # intilize a null list\n",
    "    keep_list: list[SeqRecord] = []\n",
    "\n",
    "    todo_list: list[SeqRecord] = []\n",
    "\n",
    "    already_found_list: list[str] = []\n",
    "\n",
    "    for todo in to_combine:\n",
    "        todo_list.extend(todo)\n",
    "\n",
    "    # traverse for all elements\n",
    "    for x in todo_list:\n",
    "        if x.id in already_found_list:\n",
    "          continue\n",
    "        else:\n",
    "          already_found_list.append(x.id)\n",
    "        # check if exists in unique_list or not\n",
    "        if x.id in idlist:\n",
    "            keep_list.append(x)\n",
    "\n",
    "    return keep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, records, rec_type):\n",
    "  \"\"\"\n",
    "  Save the records to the file.\n",
    "\n",
    "  `file_name`: The name of the file to save to.\n",
    "\n",
    "  `records`: The records to save.\n",
    "\n",
    "  `rec_type`: The type of record to save.\n",
    "  \"\"\"\n",
    "  file_path = os.path.join(dir_path, file_name)\n",
    "  with open(file_path, \"w\") as out_handle:\n",
    "      try:\n",
    "        count = SeqIO.write(records, out_handle, rec_type)\n",
    "        logger.debug(f\"{rec_type} {count} Saved\")\n",
    "      except UndefinedSequenceError as e_seq:\n",
    "        logger.error(e_seq)\n",
    "        logger.error(f\"Failed to write {rec_type} to file {file_name}\")\n",
    "      except Exception as e:\n",
    "        logger.error(e)\n",
    "        logger.error(f\"Failed to write {rec_type} to file {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_features(count: int, records: list[SeqRecord], rec_type: str, granularity=5, debug: bool = False):\n",
    "  \"\"\"\n",
    "  1. Extracts the features (both source and rRNA)\n",
    "  2. Based on cleaned up list of features, filter records for matching ids\n",
    "  3. Save the records to a file\n",
    "  \"\"\"\n",
    "  logger.debug(f\"Found {count} {rec_type}, sequences: {len(records)}\")\n",
    "  qualifiers: list[dict[str, str]] = []\n",
    "\n",
    "  save_records: list[SeqRecord] = []\n",
    "\n",
    "  for record in records:\n",
    "    feats: list[SeqFeature] = record.features\n",
    "\n",
    "    [source] = filter(lambda f: f.type == \"source\", feats)\n",
    "    if not source:\n",
    "       raise Exception(\"No source feature\")\n",
    "\n",
    "    # Copy the qualifiers and add the id\n",
    "    # merge the rRNA qualifiers to the same dict\n",
    "    qual = source.qualifiers.copy()\n",
    "    qual[\"id\"] = record.id\n",
    "\n",
    "    seq_desc = record.description.split(\" 16S\")\n",
    "    qual[\"sequence_name\"] = seq_desc[0]\n",
    "    record.description = seq_desc[0]\n",
    "\n",
    "    # Filter for the rRNA features\n",
    "    rRNA_filtered = list(filter(lambda f: f.type == \"rRNA\", feats))\n",
    "    if len(rRNA_filtered) == 0:\n",
    "       logger.debug(\"No rRNA feature\")\n",
    "       continue\n",
    "    elif 'contig' in record.annotations:\n",
    "      logger.debug(\"Is a contig\")\n",
    "      continue\n",
    "    else:\n",
    "      for rRNA in rRNA_filtered:\n",
    "        try:\n",
    "          qual.update(rRNA.qualifiers)\n",
    "        except Exception as e:\n",
    "           logger.error(rRNA)\n",
    "           logger.exception(e)\n",
    "\n",
    "      reference_list: list[Reference] = record.annotations.get('references', [])\n",
    "      # Get the pubmed id list from the references\n",
    "      pubmed_ids = [ref.pubmed_id for ref in reference_list if ref.pubmed_id]\n",
    "      qual['pubmed_ids'] = pubmed_ids\n",
    "\n",
    "      # Split the dbxref into a dict\n",
    "      for dbxref in record.dbxrefs:\n",
    "        split = dbxref.split(\":\")\n",
    "        if len(split) > 1:\n",
    "          qual[split[0]] = split[1]\n",
    "\n",
    "      # Since all feature values are a list of string, (usually of length 1), convert to string\n",
    "      for key in qual:\n",
    "        qual_list = qual[key]\n",
    "        if isinstance(qual_list, list):\n",
    "          qual[key] = \"; \".join(qual_list)\n",
    "\n",
    "      # Extract the colony info from the isolation_source\n",
    "      qual_iso_source = qual.get('isolation_source', '')\n",
    "      if \"Coral-associated microbial aggregate\" in qual_iso_source:\n",
    "        # logger.debug(f\"colony info found for {record.id}\")\n",
    "        iso_qual: str = qual['isolation_source']\n",
    "\n",
    "        [res] = re.findall(r'(?<=\\().+?(?=\\))', iso_qual)\n",
    "\n",
    "        qual['isolation_source'] = None\n",
    "        qual['colony'] = res\n",
    "      # If the isolation_source mentions the host, move to the info to host column instead\n",
    "      elif 'coral' in qual_iso_source or 'sponge' in qual_iso_source:\n",
    "        qual['host'] = qual_iso_source\n",
    "        qual['isolation_source'] = None\n",
    "\n",
    "      # clean up host info\n",
    "      qual_host: str = qual.get('host', '')\n",
    "      brk_results = re.findall(r\"(?<=\\().+?(?=\\))\", qual_host)\n",
    "      if len(brk_results) > 0:\n",
    "        # for cases like \"<Species name> (gorgorian coral)\", remove trailing text in the brackets\n",
    "        if \"coral\" in brk_results[0]:\n",
    "          qual['host'] = qual_host.split(\" (\")[0]\n",
    "        # For cases like \"<details> (Species name)\", extract the species name\n",
    "        else:\n",
    "          qual['host'] = brk_results[0]\n",
    "      if qual.get('host') == None:\n",
    "        qual['host'] = np.nan\n",
    "      elif qual.get('host') == '':\n",
    "        qual['host'] = np.nan\n",
    "      elif qual.get('host') and qual.get('host') != np.nan:\n",
    "        qual['host'] = qual['host'].strip().capitalize()\n",
    "        if qual['host'] == 'Sea coral':\n",
    "          qual['host'] = 'Coral'\n",
    "\n",
    "      # As a safe guard, check that the sequence is not empty\n",
    "      if record.seq:\n",
    "        sequence: Seq = record.seq\n",
    "        qual[\"sequence_length\"] = len(sequence)\n",
    "        save_records.append(record)\n",
    "\n",
    "      qualifiers.append(qual)\n",
    "\n",
    "  # Load features into a dataframe\n",
    "  df1 = DataFrame(qualifiers)\n",
    "  # Clean the table by replacing \"None\" and \"nan\"\n",
    "  df1.replace(\"None\", value=np.nan, inplace=True)\n",
    "  df1.replace(np.NaN, value=np.nan, inplace=True)\n",
    "  df1.replace(\"nan\", value=np.nan, regex=True, inplace=True)\n",
    "\n",
    "  # If not in debug mode, clean the table + save the files\n",
    "  if debug:\n",
    "    logger.debug(df1.head())\n",
    "    return df1, save_records\n",
    "  else:\n",
    "    df2 = clean_table(df1, granularity)\n",
    "    # Filter the records by the ids in the dataframe\n",
    "    filtered_recs = list(filter((lambda x: x.id in df2['id'].to_list()), save_records))\n",
    "\n",
    "    logger.info(f\"Records to save: {len(filtered_recs)}/{len(save_records)}\")\n",
    "\n",
    "    # Save the records to a file\n",
    "    save_file(f\"{rec_type}_seq.gb\", filtered_recs, \"genbank\")\n",
    "    save_file(f\"{rec_type}_seq.fasta\", filtered_recs, \"fasta\")\n",
    "\n",
    "    return df2, filtered_recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI: Popset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in NCBI (Popset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted by the NCBI's website, their `Popset database` is a collection of related sequences that is sourced from a single population/phylogenetic/mutation/ecosystem study. \n",
    "\n",
    "This is a useful grouping, as we would be able to extract the locational data from the metadata and use it to create a map of the distribution of the bacteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_popset(additional_ids: list[str] = []):\n",
    "  \"\"\"\n",
    "  Search for the popset database for the coral and Endozoicomonas.\n",
    "  :return: The number of results and the list of ids.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    term = f\"((Endozoicomonas[Organism] OR Endozoicomonas[All Fields]) AND coral[All Fields]\"\n",
    "    logger.debug(term)\n",
    "    handle = Entrez.esearch(db=\"popset\",\n",
    "      term=term,\n",
    "      retmax=20,\n",
    "    )\n",
    "    res = Entrez.read(handle)\n",
    "    if isinstance(res, DictionaryElement):\n",
    "      ids: list[str] = res[\"IdList\"]\n",
    "      ids.extend(additional_ids)\n",
    "      handle = Entrez.efetch(db=\"popset\", id=ids, rettype=\"gb\")\n",
    "      records: list[SeqRecord] = list(SeqIO.parse(handle, \"gb\"))\n",
    "      return len(records), ids, records\n",
    "    else:\n",
    "      return 0, [], []\n",
    "  except HTTPError as http_e:\n",
    "    logger.error(str(http_e))\n",
    "    return 0, [], []\n",
    "  except Exception as e:\n",
    "    logger.error(e)\n",
    "    return 0, [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After review of literature that happened in the inspection by Nucleotide DB, I found some popset that were worth adding manually.\n",
    "\n",
    "This includes the popset coming out of the work of {cite:p}`Speck_2012` (`\"227461503\"`), and the work of {cite:p}`Bayer_2013` (`\"510829312\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_ids_pop = [\n",
    "  '227461503',\n",
    "  '510829312',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_NEW:\n",
    "  count_pop, idlist_pop, records_pop = search_popset(additional_ids_pop)\n",
    "else:\n",
    "\n",
    "\n",
    "  records_pop = list(SeqIO.parse(os.path.join(dir_path, \"popset_seq.gb\"), \"gb\"))\n",
    "  count_pop = len(records_pop)\n",
    "  idlist_pop = [record.id for record in records_pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of popset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata table created for the data out of Popset database was saved to [features_of_popset_seq.csv](../../datasets/full_analysis/features_of_popset_seq.csv).\n",
    "\n",
    "Genbank file was saved as [popset_seq.gb](../../datasets/full_analysis/popset_seq.gb).\n",
    "Fasta file was saved as [popset_seq.fasta](../../datasets/full_analysis/popset_seq.fasta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:Found 2640 popset, sequences: 2640\n",
      "INFO:download_16s:Records to save: 2640/2640\n",
      "DEBUG:download_16s:genbank 2640 Saved\n",
      "DEBUG:download_16s:fasta 2640 Saved\n"
     ]
    }
   ],
   "source": [
    "# Load the metadata, and save the records to a file\n",
    "feat_df_pop0, recs_pop = table_features(count_pop, records_pop, \"popset\", granularity=GRANULARITY, debug=False)\n",
    "\n",
    "feat_df_pop0.to_csv(os.path.join(dir_path, \"features_of_popset_seq.csv\"), index=False)\n",
    "\n",
    "feat_df_pop = feat_df_pop0.copy()\n",
    "col_interest = ['id', 'sequence_name', 'group', 'organism', 'strain', 'host', 'country', 'taxon', 'BioProject', 'pubmed_ids', 'isolation_source', 'product']\n",
    "feat_df_pop = feat_df_pop[col_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country       isolation_source          host                  \n",
       "Bahamas       Tuna Alley (inside reef)  Plexaura sp.                1\n",
       "France        NaN                       Corallium rubrum            1\n",
       "Malaysia      NaN                       Coral reef ecosystems       5\n",
       "Saudi Arabia  Southern Red Sea          Acropora humilis          322\n",
       "                                        Pocillopora damicornis    496\n",
       "                                        Stylophora pistillata     354\n",
       "Taiwan        Green Island              Stylophora pistillata      17\n",
       "              Kenting                   Stylophora pistillata      21\n",
       "              Yeh Liu                   Stylophora pistillata       1\n",
       "USA           Hillsboro Ledge           Eunicea fusca               1\n",
       "              Summerland Key, Florida   Plexaura homomalla         25\n",
       "              West Maui                 Porites compressa         758\n",
       "NaN           NaN                       Coral                      58\n",
       "                                        Coral mucus                 4\n",
       "                                        Eunicella cavolini         20\n",
       "                                        Stylophora pistillata     556\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df_pop.groupby(['country', 'isolation_source','host'], dropna=False)['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI: Nucleotide databse\n",
    "\n",
    "While **+1500** results is absolute enough to work with, I will expand the scope to include more sequences by searching directly through the nucleotide database.\n",
    "\n",
    "I will combine the popset dataset and the findings below, and then remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in NCBI (Nucleotide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review of literature revealed one nucleotide sequence that was not included by the search using `search_nucleo(...)` function, which found a match to a coral species found in the Red Sea {cite}`Pogoreutz2022`. We will manually note this, and add it to our list of sequences to download. \n",
    "\n",
    "\n",
    "Interestingly, looking more into the referenced sequence revealed that the original study had a total of 412 16S Endozoicomonas sequences to work with {cite}`Bayer_2013`. This will be added by adding the accession numbers by Popset (above).\n",
    "\n",
    "Additionally, they {cite}`Bayer_2013` also provided a supplementary phylogenetic tree. We will use this to add more accession ids to our list of sequences to download.\n",
    "\n",
    "```{figure} ../../outputs/full_analysis/bayer_2013_fig_s3_1pg_rotated.png\n",
    "---\n",
    "name: parsimony-tree-bayer-2013\n",
    "---\n",
    "Tree showing the phylogenetic relationship of Endozoicomonas to other bacterial species.\n",
    "```\n",
    "\n",
    "These are the accession IDs mentioned in the tree above.\n",
    "| Accession ID | Status | Host | DOI |\n",
    "|--------------|------- |----- |---- |\n",
    "| GU118644  | Probable | Montastraea faveolata (coral) | {cite}`Sunagawa_2010` |\n",
    "| GU118168  | Probable | Diploria strigosa (coral) |{cite}`Sunagawa_2010` |\n",
    "| GU118379  | Probable | Gorgonia ventalina (coral) |{cite}`Sunagawa_2010` |\n",
    "| GU118072  | Probable | Acropora palmata (coral) |{cite}`Sunagawa_2010` |\n",
    "| GU118404  | Probable | Gorgonia ventalina (coral) |{cite}`Sunagawa_2010` |\n",
    "| GU118957  | Probable | Porites astreoides (coral) |{cite}`Sunagawa_2010` |\n",
    "| GU784983  | Probable | Sponge Ianthella basta |{cite}`Luter_2010` |\n",
    "| GU118966  | Probable | Porites astreoides (coral) | {cite}`Sunagawa_2010` |\n",
    "| AB695088  | Probable | Haplosclerida gen. et sp. (purple sponge) | {cite}`Nishijima_2013` |\n",
    "| AM259915  | Probable | Chondrilla nucula (sponge) | {cite}`Thiel_2007` |\n",
    "| DQ884169  | Probable (Uncultured Gammaproteobacteria) | Cystodytes dellechiajei (tunicate) | {cite}`MartinezGarcia2006` |\n",
    "| DQ884170  | Probable (Gammaproteobacteria) | Cystodytes dellechiajei (tunicate) | {cite}`MartinezGarcia2006` |\n",
    "| DQ884160  | Probable (Uncultured Gammaproteobacteria) | Cystodytes dellechiajei (tunicate) | {cite}`MartinezGarcia2006` | \n",
    "| DQ917901  | Probable | Muricea elongata (Octocoral) | {cite}`Ranzer2007` |\n",
    "| AY700600  | Probable | Pocillopora damicornis (coral) | {cite}`Bourne_2005` |\n",
    "| AY700601  | Probable | Pocillopora damicornis (coral) | {cite}`Bourne_2005` |\n",
    "| FJ202634  | Probable, (uncultured bacterium) | Montastraea faveolata (coral) | {cite}`Sunagawa_2009` |\n",
    "| FJ347758  | Probable | Montipora aequituberculata (coral) | {cite}`Yang_2010` | \n",
    "| FJ930289  | Probable, (Uncultured bacterium) | Porites compressa (coral) | {cite}`Speck_2012` |\n",
    "\n",
    "> `DQ884169  (C19)` `DQ884170 (C23)`, `DQ884160 (CRNA5)` noted with \"colony sequences C19, C23 and CRNA5 [...] formed a group related to Endozoiciomonas elysicola (97.6–99.7% similarity), a bacterium isolated from the marine mollusc Elysia ornata (M. Kurahashi, unpublished) {cite}`MartinezGarcia2006`\"\n",
    "\n",
    "\n",
    "From above tree, but will not be included.\n",
    "| Accession ID | Status | Host | DOI |\n",
    "|--------------|------- |----- |---- |\n",
    "| AM503093  | Very unlikely (Marinobacter guineae) | Antarctic environment |  |\n",
    "| AM229315  | Very unlikely (Halomonas janggokensis) | saline water |  |\n",
    "| GU291858  | Unlikely (due to distance on tree) | solar saltern | {cite}`Joung2010` |\n",
    "| AB205011  | Unlikely, Spongiobacter nickelotolerans | marine sponge | |\n",
    "| AB196667  | Probable, not included | Elysia ornata (sea slug) | {cite}`Kurahashi_2007` |\n",
    "| DQ917830  | Unlikely (Spongiobacter) | Cystodytes dellechiajei (tunicate) | {cite}`MartinezGarcia2006` |\n",
    "| DQ917877  | Unlikely (Spongiobacter) | Cystodytes dellechiajei (tunicate) | {cite}`MartinezGarcia2006` |\n",
    "| DQ917879  | Unlikely (Spongiobacter) | Muricea elongata (Octocoral) | {cite}`Ranzer2007` |\n",
    "| GQ853555  | Probable, not included | Loripes lacteus (clam) | gill symbiont | {cite}`Mausz2010` |\n",
    "| FM162182  | Probable, not included | Bathymodiolus brooksi (mussels) | {cite}`Zielinski_2009` |\n",
    "| FM163188  | Probable, not included | Bathymodiolus brooksi (mussels) | {cite}`Zielinski_2009` |\n",
    "| FJ154998  | Probable, (uncultured bacterium) not included | ocean water | | \n",
    "| EU884930  | Probable, not included | Sixbar angelfish | |\n",
    "\n",
    "Here are other accession IDs that were found when examining the references of the papers above.\n",
    "| Accession ID | DOI |\n",
    "|--------------|-----|\n",
    "| AB695089 | {cite}`Nishijima_2013` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nucleo(limit: int, additional_ids: list[str] = []):\n",
    "  \"\"\"\n",
    "  Search for the nucleotide database for nucleotides that mention coral and Endozoicomonas.\n",
    "\n",
    "  :param `limit`: The maximum number of results to return.\n",
    "  \"\"\"\n",
    "\n",
    "  term = \"(coral[All Fields] AND 16S[Title]) AND Endozoicomonas[Organism]\"\n",
    "  try:\n",
    "    handle = Entrez.esearch(db=\"nucleotide\",\n",
    "      term=term,\n",
    "      retmax=limit,\n",
    "    )\n",
    "    res = Entrez.read(handle)\n",
    "    if isinstance(res, DictionaryElement):\n",
    "      ids: list[str] = res[\"IdList\"]\n",
    "      ids.extend(additional_ids)\n",
    "      logger.debug(ids)\n",
    "      handle = Entrez.efetch(db=\"nucleotide\", id=ids, rettype=\"gb\")\n",
    "      records: list[SeqRecord] = list(SeqIO.parse(handle, \"gb\"))\n",
    "      return len(records), ids, records\n",
    "    else:\n",
    "      return 0, [], []\n",
    "  except HTTPError as http_e:\n",
    "    logger.error(http_e.read())\n",
    "    logger.exception(http_e)\n",
    "    return 0, [], []\n",
    "  except Exception as e:\n",
    "    logger.error(e)\n",
    "    logger.exception(e)\n",
    "    return 0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_accessions = ['AB695089', 'HE818343']\n",
    "if SEARCH_NEW:\n",
    "  count_nuc, idlist_nuc, records_nuc = search_nucleo(4000, additional_accessions)\n",
    "else:\n",
    "  records_nuc = list(SeqIO.parse(os.path.join(dir_path, \"nucleotide_seq.gb\"), \"gb\"))\n",
    "  count_nuc = len(records_nuc)\n",
    "  idlist_nuc = [record.id for record in records_pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of nucleotide database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the...\n",
    "| Type of data | File name |\n",
    "|--------------|-----------|\n",
    "| Metadata | [features_of_nucleotide_seq.csv](../../datasets/full_analysis/features_of_nucleotide_seq.csv) |\n",
    "| Genbank | [nucleotide_seq.gb](../../datasets/full_analysis/nucleotide_seq.gb) |\n",
    "| Fasta | [nucleotide_seq.fasta](../../datasets/full_analysis/nucleotide_seq.fasta) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything combined (nucleotide + popset)\n",
    "\n",
    "We will save the...\n",
    "| Type of data | File name |\n",
    "|--------------|-----------|\n",
    "| Metadata | [feat_all_seq.csv](../../datasets/full_analysis/feat_all_seq.csv) |\n",
    "| Genbank | [all_seq.gb](../../datasets/full_analysis/all_seq.gb) |\n",
    "| Fasta | [all_seq.fasta](../../datasets/full_analysis/all_seq.fasta) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_excel\n",
    "\n",
    "latlng_file = read_excel(os.path.join(dir_path, \"../../datasets/full_analysis/handwritten_latlng.xlsx\"), sheet_name=\"Sheet1\")\n",
    "latlng_file['country'] = latlng_file['country'].astype(str)\n",
    "# switch every \"nan\" to np.nan\n",
    "latlng_file.replace(\"nan\", value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel file\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from pandas import to_datetime\n",
    "\n",
    "\n",
    "def annotate_groups(df0: DataFrame):\n",
    "  df = df0.copy()\n",
    "  # First, saudi arabia\n",
    "  # Fix group KC669\n",
    "\n",
    "  is_acropora_h = df['host'] == 'Acropora humilis'\n",
    "  is_pocillopora_d = df['host'] == 'Pocillopora damicornis'\n",
    "\n",
    "  df.loc[(df['country'] == 'Saudi Arabia') & is_acropora_h, 'latlng'] = \"19°52'24.48'N, 40°04'46.14'E\"\n",
    "  df.loc[(df['country'] == 'Saudi Arabia') & is_acropora_h, 'isolation_source'] = \"Red Sea, Brown reef\"\n",
    "  df.loc[(df['country'] == 'Saudi Arabia') & is_pocillopora_d, 'latlng'] = \"19°52'24.48'N, 40°04'46.14'E\"\n",
    "  df.loc[(df['country'] == 'Saudi Arabia') & is_pocillopora_d, 'isolation_source'] = \"Red Sea, Brown reef\"\n",
    "  df.loc[is_pocillopora_d | is_acropora_h, 'collection_date'] = date(2010, 7, 1)\n",
    "\n",
    "  is_stylopora = df['host'] == 'Stylophora pistillata'\n",
    "\n",
    "  is_site_5 = df['clone'].str.contains('05')\n",
    "  site_5_ltlng = \"18°40'30.36'N, 40°44'21.18'E\"\n",
    "\n",
    "  is_site_12 = df['clone'].str.contains('12')\n",
    "  site_12_ltlng = \"18°40'30.36'N, 40°44'21.18'E\"\n",
    "\n",
    "  is_site_14 = df['clone'].str.contains('14')\n",
    "  site_14_ltlng = \"19°53'52.74'N, 40° 0'53.46'E\"\n",
    "\n",
    "  is_site_15 = df['clone'].str.contains('15')\n",
    "  site_15_ltlng = \"19°53'15.42'N, 40°9'23.94'E\"\n",
    "\n",
    "  is_site_17 = df['clone'].str.contains('17')\n",
    "  site_17_ltlng = \"20°8'58.38'N, 40°14'7.50'E\"\n",
    "  is_kc669_668 = df['id'].str.contains('KC669') | df['id'].str.contains('KC668')\n",
    "  df.loc[is_kc669_668, 'country'] = 'Saudi Arabia'\n",
    "  df.loc[is_kc669_668 | is_stylopora, 'collection_date'] = date(2009, 6, 1)\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_5, 'latlng'] = site_5_ltlng\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_12, 'latlng'] = site_12_ltlng\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_14, 'latlng'] = site_14_ltlng\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_15, 'latlng'] = site_15_ltlng\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_17, 'latlng'] = site_17_ltlng\n",
    "  df.loc[is_kc669_668 & is_stylopora, 'isolation_source'] = \"Southern Red Sea\"\n",
    "\n",
    "  # remove extra maui\n",
    "  is_Maui = df['isolation_source'] == 'West Maui'\n",
    "\n",
    "  has_c7 = df['clone'] == 'C7-A01c'\n",
    "  keep_df = df.loc[is_Maui & has_c7].copy()\n",
    "\n",
    "  df = df.loc[~is_Maui]\n",
    "  df = concat([df, keep_df])\n",
    "\n",
    "  # deal with australia\n",
    "  df.loc[df['id'].str.contains('KM3604'), 'country'] = 'Australia'\n",
    "  df.loc[df['id'].str.contains('KM3604'), 'isolation_source'] = 'Orpheus Island Great Barrier Reef'\n",
    "\n",
    "  # deal with taiwan\n",
    "  is_taiwan_group = df['id'].str.contains('JN635')\n",
    "  is_green_island = df['isolation_source'] == 'Green Island'\n",
    "  # Split the clone column\n",
    "  df['clone_num'] = df.loc[is_taiwan_group & is_green_island]['clone'].str.split('U', expand=True)[1].astype(int)\n",
    "  # if the clone num is less than 1000, it's a Kunguan group\n",
    "  is_kunguan_group = df['clone_num'] < 1000\n",
    "  # otherwise it's a Chaikou, Green Island\n",
    "  is_chaikou_group = df['clone_num'] >= 1000\n",
    "  df.loc[is_taiwan_group & is_green_island & is_kunguan_group, 'isolation_source'] = \"Kunguan, Green Island\"\n",
    "  df.loc[is_taiwan_group & is_green_island & is_chaikou_group, 'isolation_source'] = \"Chaikou, Green Island\"\n",
    "\n",
    "  # Remove some of the extra taiwan\n",
    "  to_remove = ['NR_116609.1', 'NR_158127.1', 'NR_169415.1']\n",
    "  df = df.loc[~df['id'].isin(to_remove)]\n",
    "\n",
    "  for index, row in latlng_file.iterrows():\n",
    "    group = row['group']\n",
    "    id_is_group = df['id'].astype(str).str.contains(group)\n",
    "    df.loc[id_is_group, 'doi'] = row['doi']\n",
    "\n",
    "    country = row['country']\n",
    "\n",
    "    if isinstance(country, str):\n",
    "      df.loc[id_is_group & (df['country'].isna()), 'country'] = row['country']\n",
    "\n",
    "    if group != 'JN635':\n",
    "      df.loc[id_is_group, 'collection_date'] = row['date']\n",
    "      df.loc[id_is_group, 'latlng'] = row['latlng']\n",
    "      df.loc[id_is_group, 'isolation_source'] = row['source']\n",
    "\n",
    "    else:\n",
    "      find_matching_taiwan = id_is_group & (df['isolation_source'] == row['source'])\n",
    "      df.loc[find_matching_taiwan, 'collection_date'] = row['date']\n",
    "      df.loc[find_matching_taiwan, 'latlng'] = row['latlng']\n",
    "      df.loc[find_matching_taiwan, 'isolation_source'] = row['source']\n",
    "\n",
    "  # clean up\n",
    "  df.replace(\"None\", value=np.nan, inplace=True)\n",
    "  df.replace(np.NaN, value=np.nan, inplace=True)\n",
    "  df.replace(\"nan\", value=np.nan, regex=True, inplace=True)\n",
    "  df.replace(\"NaN\", value=np.nan, regex=True, inplace=True)\n",
    "  df.replace(\"\", value=np.nan, inplace=True)\n",
    "\n",
    "  df['collection_date'] = to_datetime(df['collection_date'])\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lat_lon_parser import parse\n",
    "\n",
    "def add_decimal_latlng(df0: DataFrame):\n",
    "  df = df0.copy()\n",
    "  df[['lat','long']] = df['latlng'].str.split(',',expand=True)\n",
    "  df['lat'].str.strip()\n",
    "  df['long'].str.strip()\n",
    "\n",
    "  df['lat_dec'] = df.apply(lambda row: parse(row['lat']), axis=1)\n",
    "  df['long_dec'] = df.apply(lambda row: parse(row['long']), axis=1)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:Found 155 nucleotide, sequences: 155\n",
      "INFO:download_16s:Records to save: 155/155\n",
      "DEBUG:download_16s:genbank 155 Saved\n",
      "DEBUG:download_16s:fasta 155 Saved\n"
     ]
    }
   ],
   "source": [
    "feat_df_nuc0, recs_nuc = table_features(count_nuc, records_nuc, \"nucleotide\", granularity=GRANULARITY, debug=False)\n",
    "feat_df_nuc0.to_csv(os.path.join(dir_path, \"features_of_nucleotide_seq.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Records found: 1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Records found: 1095\n"
     ]
    }
   ],
   "source": [
    "feat_df_all = concat([feat_df_nuc0, feat_df_pop0]).drop_duplicates(subset=\"id\")\n",
    "\n",
    "logger.info(f\"Records found: {len(feat_df_all['id'].to_list())}\")\n",
    "feat_df_all = annotate_groups(feat_df_all)\n",
    "\n",
    "logger.info(f\"Records found: {len(feat_df_all['id'].to_list())}\")\n",
    "\n",
    "feat_df_all = add_decimal_latlng(feat_df_all)\n",
    "\n",
    "col_interest = ['id', 'country', 'group', 'host', 'isolation_source', 'sequence_length', 'sequence_name', 'collection_date', 'latlng', 'lat_dec', 'long_dec']\n",
    "# ['id', 'clone', 'sequence_name', 'group', 'organism', 'host', 'country', 'pubmed_ids', 'isolation_source','sequence_length']\n",
    "feat_df_all = feat_df_all[col_interest].sort_values(by=['sequence_length'], ascending=False).sort_values(by=['id']).reset_index(drop=True)\n",
    "feat_df_all.to_csv(os.path.join(dir_path, \"feat_all_seq.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Unique records found: 1095 / 1095\n",
      "DEBUG:download_16s:genbank 1095 Saved\n",
      "DEBUG:download_16s:fasta 1095 Saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'country', 'group', 'host', 'isolation_source', 'sequence_length',\n",
      "       'sequence_name', 'collection_date', 'latlng', 'lat_dec', 'long_dec'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "unique_recs = unique([recs_nuc, recs_pop], feat_df_all)\n",
    "logger.info(f\"Unique records found: {len(unique_recs)} / {len(feat_df_all['id'].to_list())}\")\n",
    "# Save the records to a file\n",
    "save_file(\"all_seq.gb\", unique_recs, \"genbank\")\n",
    "save_file(\"all_seq.fasta\", unique_recs, \"fasta\")\n",
    "print(feat_df_all.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic details\n",
    "\n",
    "Here shows the breakdown by country, and by isolation source as reported by the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country       isolation_source                   \n",
       "Australia     Orpheus Island Great Barrier Reef        3\n",
       "Bahamas       Bimini, Tuna Alley                       1\n",
       "France        Bay of Villefranche-sur-Mer             20\n",
       "              Porticcio, Corsica Island                1\n",
       "Japan         Okinawa                                 39\n",
       "              Shizuoka, Numazu                         1\n",
       "Kuwait        Qit'at Bnaider                           1\n",
       "Malaysia      Bidong Island, Terengganu                5\n",
       "Portugal      Algarve, Gale Alta, Armacao de Pera      1\n",
       "Saudi Arabia  Al-Fahal reef                           58\n",
       "              Red Sea                                  2\n",
       "              Red Sea, Brown reef                    409\n",
       "              Southern Red Sea                       455\n",
       "Taiwan        Chaikou, Green Island                   10\n",
       "              Kenting                                 52\n",
       "              Kunguan, Green Island                    7\n",
       "              Yeh Liu                                  1\n",
       "USA           Florida                                  1\n",
       "              Florida Keys, Florida                    2\n",
       "              Summerland Key, Florida                 25\n",
       "              West Maui patch reef                     1\n",
       "Name: isolation_source, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df_all.groupby(['country', 'isolation_source'], dropna=False)['isolation_source'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country       group  host                       isolation_source                   \n",
       "Australia     KM360  Coral mucus                Orpheus Island Great Barrier Reef        3\n",
       "Bahamas       JX488  Plexaura sp.               Bimini, Tuna Alley                       1\n",
       "France        JQ691  Eunicella cavolini         Bay of Villefranche-sur-Mer             20\n",
       "              KT964  Corallium rubrum           Porticcio, Corsica Island                1\n",
       "Japan         AB695  Haplosclerida gen. et sp.  Shizuoka, Numazu                         1\n",
       "              OL957  Stylophora pistillata      Okinawa                                 39\n",
       "Kuwait        HM804  Coral mucus                Qit'at Bnaider                           1\n",
       "Malaysia      MG896  Coral reef ecosystems      Bidong Island, Terengganu                5\n",
       "Portugal      HE818  Marine sponge              Algarve, Gale Alta, Armacao de Pera      1\n",
       "Saudi Arabia  KC668  Acropora humilis           Red Sea, Brown reef                    161\n",
       "                     Pocillopora damicornis     Red Sea, Brown reef                    248\n",
       "                     Stylophora pistillata      Southern Red Sea                       177\n",
       "              KC669  Stylophora pistillata      Southern Red Sea                       278\n",
       "              MG725  Coral                      Al-Fahal reef                           58\n",
       "              MW540  Acropora cytherea          Red Sea                                  1\n",
       "                     Acropora hemprichii        Red Sea                                  1\n",
       "Taiwan        FJ347  Coral                      Kenting                                  1\n",
       "              JN635  Stylophora pistillata      Chaikou, Green Island                   10\n",
       "                                                Kenting                                 21\n",
       "                                                Kunguan, Green Island                    7\n",
       "              JN848  Stylophora pistillata      Yeh Liu                                  1\n",
       "              LN875  Coral                      Kenting                                  1\n",
       "              LN879  Coral                      Kenting                                  1\n",
       "              OL957  Stylophora pistillata      Kenting                                 28\n",
       "USA           FJ930  Porites compressa          West Maui patch reef                     1\n",
       "              JX488  Eunicea fusca              Florida                                  1\n",
       "              KF428  Plexaura homomalla         Summerland Key, Florida                 25\n",
       "              OM273  Eunicea flexuosa           Florida Keys, Florida                    2\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouping = ['country', 'group', 'host', 'isolation_source']\n",
    "feat_df_all.groupby(grouping, dropna=False)['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tip_dates(df0: DataFrame, file_name: str):\n",
    "  df = df0.drop(['latlng'], axis=1)\n",
    "  print(len(df))\n",
    "  # replace space with underscore\n",
    "  df['seq_name2'] = df['sequence_name'].str.replace(' ', '_')\n",
    "  df['full_name'] = df[['id', 'seq_name2']].agg(' '.join, axis=1)\n",
    "  df['collection_date_str'] = df['collection_date'].dt.strftime('%Y-%m-%d')\n",
    "  print(len(df))\n",
    "  df[['full_name', 'collection_date_str']].to_csv(os.path.join(dir_path, \"../../outputs/full_analysis\", file_name), index=False, sep=\"\\t\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trait_values(df0: DataFrame, file_name: str):\n",
    "  df = df0.drop(['latlng'], axis=1)\n",
    "  df['seq_name2'] = df['sequence_name'].str.replace(' ', '_')\n",
    "  df['collection_date_str'] = df['collection_date'].dt.strftime('%Y-%m-%d')\n",
    "  df['full_name'] = df[['id', 'seq_name2', 'collection_date_str']].agg(' '.join, axis=1)\n",
    "\n",
    "  # rename lat_dec and long_dec to lat and long\n",
    "  df.rename(columns={'lat_dec': 'lat', 'long_dec': 'long'}, inplace=True)\n",
    "\n",
    "  print(len(df))\n",
    "  df[['full_name', 'lat', 'long']].to_csv(os.path.join(dir_path, \"../../outputs/full_analysis\", file_name), index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "to_align_df = feat_df_all.groupby(grouping, dropna=False).head(10)\n",
    "save_trait_values(to_align_df, \"140_trait_values.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:fasta 140 Saved\n"
     ]
    }
   ],
   "source": [
    "to_align = [rec for rec in unique_recs if rec.id in to_align_df['id'].to_list()]\n",
    "\n",
    "save_file(\"to_align.fasta\", to_align, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align the 16S sequences\n",
    "\n",
    "Use muscle to align the sequences.\n",
    "\n",
    "```bash\n",
    "muscle5.1 -align ./datasets/full_analysis/to_align.fasta -output ./outputs/full_analysis/muscle/aligned_top_140.fasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the descrption in the fasta file to make using the file in BEAST easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment length 1722\n",
      "Number of records 140\n"
     ]
    }
   ],
   "source": [
    "# Read aligned fasta file\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from datetime import datetime\n",
    "\n",
    "alignment = AlignIO.read(os.path.realpath(\"..\\\\..\\\\outputs\\\\full_analysis\\\\muscle\\\\aligned_top_140.fasta\"), \"fasta\")\n",
    "if isinstance(alignment, MultipleSeqAlignment):\n",
    "  print(f\"Alignment length {alignment.get_alignment_length()}\")\n",
    "  print(f\"Number of records {len(alignment)}\")\n",
    "  for seq in alignment:\n",
    "    arr_desc = str(seq.description).split(\" \")\n",
    "    seq_id : str = seq.id\n",
    "    date: datetime = to_align_df.loc[to_align_df['id'] == seq_id, 'collection_date'].iloc[0]\n",
    "    # convert the date to a string\n",
    "    seq.description = \" \".join([\"_\".join(arr_desc[1:]), date.strftime(\"%Y-%m-%d\")])\n",
    "\n",
    "  # Save the alignment to a file\n",
    "  AlignIO.write(alignment, os.path.realpath(\"../../outputs/full_analysis/muscle/140_aligned_name_replaced.fasta\"), \"fasta\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
