{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download: 16S rRNA sequences\n",
    "\n",
    "Since 16S shows more promising results of identifying phylogeny of bacterial taxa, I will download a new set, but specifically look at entries under \"popset\" first. If there aren't enough sequences, I will expand the scope to include more sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "We will utilize the package [`BioPython`](https://biopython.org/wiki/Documentation) {cite}`Cock_2009` to interface with sequencial data hosted on NCBI. We will also use `pandas` {cite}`pandasdevelopmentteam2023, McKinney2010` to store metadata in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.SeqFeature import SeqFeature, Reference\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq, UndefinedSequenceError\n",
    "from Bio.Entrez.Parser import DictionaryElement\n",
    "from Bio.Entrez import HTTPError\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame, notna, merge, concat, read_csv, to_datetime, read_excel\n",
    "import numpy as np\n",
    "from lat_lon_parser import parse\n",
    "from datetime import date\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = \"kaedeito@student.ubc.ca\"\n",
    "# This line sets the name of the tool that is making the queries\n",
    "Entrez.tool = \"download_16s.ipynb\"\n",
    "\n",
    "SEARCH_NEW = False\n",
    "GRANULARITY = 5\n",
    "logger = logging.getLogger(\"download_16s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder to save our fasta and genbank file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath(\"..\\\\..\\\\datasets\\\\full_analysis\")\n",
    "os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and wrangle tables\n",
    "\n",
    "We need to clean the data to make it easier to work with.\n",
    "We need to load the metadata into a tabular format to make it easier to find problems, or to find interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table(df: DataFrame, granularity:int):\n",
    "  \"\"\"\n",
    "  Clean the dataframe by removing rows and cleaning string formatting.\n",
    "\n",
    "  :param `df`: The dataframe to clean.\n",
    "  :param `granularity`: The number of characters to group the ids by.\n",
    "  \"\"\"\n",
    "  feat_df = df.copy()\n",
    "\n",
    "  # Split the country column into two columns\n",
    "  feat_df[['country','iso_source2']] = feat_df['country'].str.split(':',expand=True)\n",
    "  # Trim whitespace from the isolation_source column\n",
    "  feat_df['iso_source2'] = feat_df['iso_source2'].map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "  # Group the ids by the first n characters\n",
    "  feat_df['group'] = feat_df['id'].str[:granularity]\n",
    "\n",
    "  # Fill the isolation_source column with the iso_source2 column if it is null\n",
    "  feat_df['isolation_source'] = feat_df['isolation_source'].fillna(feat_df['iso_source2'])\n",
    "\n",
    "  feat_df.loc[feat_df['group'] == 'KC668', 'country'] = 'Saudi Arabia'\n",
    "  feat_df.loc[feat_df['group'] == 'KC668', 'isolation_source'] = 'Southern Red Sea'\n",
    "\n",
    "  # Print all that do not have Endozoi\n",
    "  # Clean the table by removing the rows that do not contain Endozoicomonas / approved other taxa\n",
    "\n",
    "  not_endo_df = feat_df[notna(feat_df['organism']) & ~(feat_df['organism'].astype(str).str.contains('Endozoicomonas|uncultured bacterium', regex=True))]\n",
    "  not_endo_df.to_csv(os.path.join(dir_path, 'not_endo.csv'), index=False)\n",
    "\n",
    "  # feat_df = feat_df[notna(feat_df['organism']) & (feat_df['organism'].astype(str).str.contains('Endozoicomonas|uncultured bacterium', regex=True))]\n",
    "\n",
    "  # Clean the table by removing the rows that do not contain 16S\n",
    "  feat_df = feat_df[notna(feat_df['product']) & (feat_df['product'].astype(str).str.contains('16S'))]\n",
    "\n",
    "  # Copy the colony info from the isolation_source column\n",
    "  if 'colony' in feat_df.columns:\n",
    "    feat_df['strain'] = feat_df['strain'].fillna(feat_df['colony'])\n",
    "\n",
    "  if 'BioProject' not in feat_df.columns:\n",
    "    feat_df['BioProject'] = np.nan\n",
    "\n",
    "  # Split the db_xref column into multiple columns, and use the first column as column name\n",
    "  feat_df_nuc_xref = feat_df[['id', 'db_xref']].copy()\n",
    "  feat_df_nuc_xref[['xref','value']] = feat_df['db_xref'].str.split(':',expand=True)\n",
    "\n",
    "  feat_df_nuc_xref = feat_df_nuc_xref.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "  feat_df_nuc_xref = feat_df_nuc_xref.pivot(index=['id'], columns='xref', values='value')\n",
    "  feat_df = merge(feat_df, feat_df_nuc_xref, on=\"id\")\n",
    "\n",
    "  # replace every \"Ken Ting\" with \"Kenting\"\n",
    "  feat_df['isolation_source'].replace('Ken Ting', 'Kenting', inplace=True)\n",
    "\n",
    "  # remove the columns that are not needed\n",
    "  feat_df.drop(columns=['db_xref'], inplace=True)\n",
    "\n",
    "  # Reset the index\n",
    "  feat_df.reset_index()\n",
    "\n",
    "  return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(to_combine: list[list[SeqRecord]], all_df: DataFrame):\n",
    "    idlist = all_df['id'].to_list()\n",
    "    # intilize a null list\n",
    "    keep_list: list[SeqRecord] = []\n",
    "\n",
    "    todo_list: list[SeqRecord] = []\n",
    "\n",
    "    already_found_list: list[str] = []\n",
    "\n",
    "    for todo in to_combine:\n",
    "        todo_list.extend(todo)\n",
    "\n",
    "    # traverse for all elements\n",
    "    for x in todo_list:\n",
    "        if x.id in already_found_list:\n",
    "          continue\n",
    "        else:\n",
    "          already_found_list.append(x.id)\n",
    "        # check if exists in unique_list or not\n",
    "        if x.id in idlist:\n",
    "            keep_list.append(x)\n",
    "\n",
    "    return keep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, records, rec_type):\n",
    "  \"\"\"\n",
    "  Save the records to the file.\n",
    "\n",
    "  `file_name`: The name of the file to save to.\n",
    "\n",
    "  `records`: The records to save.\n",
    "\n",
    "  `rec_type`: The type of record to save.\n",
    "  \"\"\"\n",
    "  file_path = os.path.join(dir_path, file_name)\n",
    "  with open(file_path, \"w\") as out_handle:\n",
    "      try:\n",
    "        count = SeqIO.write(records, out_handle, rec_type)\n",
    "        logger.debug(f\"{rec_type} {count} Saved\")\n",
    "      except UndefinedSequenceError as e_seq:\n",
    "        logger.error(e_seq)\n",
    "        logger.error(f\"Failed to write {rec_type} to file {file_name}\")\n",
    "      except Exception as e:\n",
    "        logger.error(e)\n",
    "        logger.error(f\"Failed to write {rec_type} to file {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_features(count: int, records: list[SeqRecord], rec_type: str, granularity=5, debug: bool = False):\n",
    "  \"\"\"\n",
    "  1. Extracts the features (both source and rRNA)\n",
    "  2. Based on cleaned up list of features, filter records for matching ids\n",
    "  3. Save the records to a file\n",
    "  \"\"\"\n",
    "  logger.debug(f\"Found {count} {rec_type}, sequences: {len(records)}\")\n",
    "  qualifiers: list[dict[str, str]] = []\n",
    "\n",
    "  save_records: list[SeqRecord] = []\n",
    "\n",
    "  for record in records:\n",
    "    feats: list[SeqFeature] = record.features\n",
    "\n",
    "    [source] = filter(lambda f: f.type == \"source\", feats)\n",
    "    if not source:\n",
    "       raise Exception(\"No source feature\")\n",
    "\n",
    "    # Copy the qualifiers and add the id\n",
    "    # merge the rRNA qualifiers to the same dict\n",
    "    qual = source.qualifiers.copy()\n",
    "    qual[\"id\"] = record.id\n",
    "\n",
    "    seq_desc = record.description.split(\" 16S\")\n",
    "    qual[\"sequence_name\"] = seq_desc[0]\n",
    "    record.description = seq_desc[0]\n",
    "\n",
    "    # Filter for the rRNA features\n",
    "    rRNA_filtered = list(filter(lambda f: f.type == \"rRNA\", feats))\n",
    "    if len(rRNA_filtered) == 0:\n",
    "       logger.debug(\"No rRNA feature\")\n",
    "       continue\n",
    "    elif 'contig' in record.annotations:\n",
    "      logger.debug(\"Is a contig\")\n",
    "      continue\n",
    "    else:\n",
    "      for rRNA in rRNA_filtered:\n",
    "        try:\n",
    "          qual.update(rRNA.qualifiers)\n",
    "        except Exception as e:\n",
    "           logger.error(rRNA)\n",
    "           logger.exception(e)\n",
    "\n",
    "      reference_list: list[Reference] = record.annotations.get('references', [])\n",
    "      # Get the pubmed id list from the references\n",
    "      pubmed_ids = [ref.pubmed_id for ref in reference_list if ref.pubmed_id]\n",
    "      qual['pubmed_ids'] = pubmed_ids\n",
    "\n",
    "      # Split the dbxref into a dict\n",
    "      for dbxref in record.dbxrefs:\n",
    "        split = dbxref.split(\":\")\n",
    "        if len(split) > 1:\n",
    "          qual[split[0]] = split[1]\n",
    "\n",
    "      # Since all feature values are a list of string, (usually of length 1), convert to string\n",
    "      for key in qual:\n",
    "        qual_list = qual[key]\n",
    "        if isinstance(qual_list, list):\n",
    "          qual[key] = \"; \".join(qual_list)\n",
    "\n",
    "      # Extract the colony info from the isolation_source\n",
    "      qual_iso_source = qual.get('isolation_source', '')\n",
    "      if \"Coral-associated microbial aggregate\" in qual_iso_source:\n",
    "        # logger.debug(f\"colony info found for {record.id}\")\n",
    "        iso_qual: str = qual['isolation_source']\n",
    "\n",
    "        [res] = re.findall(r'(?<=\\().+?(?=\\))', iso_qual)\n",
    "\n",
    "        qual['isolation_source'] = None\n",
    "        qual['colony'] = res\n",
    "      # If the isolation_source mentions the host, move to the info to host column instead\n",
    "      elif 'coral' in qual_iso_source or 'sponge' in qual_iso_source:\n",
    "        qual['host'] = qual_iso_source\n",
    "        qual['isolation_source'] = None\n",
    "\n",
    "      # clean up host info\n",
    "      qual_host: str = qual.get('host', '')\n",
    "      brk_results = re.findall(r\"(?<=\\().+?(?=\\))\", qual_host)\n",
    "      if len(brk_results) > 0:\n",
    "        # for cases like \"<Species name> (gorgorian coral)\", remove trailing text in the brackets\n",
    "        if \"coral\" in brk_results[0]:\n",
    "          qual['host'] = qual_host.split(\" (\")[0]\n",
    "        # For cases like \"<details> (Species name)\", extract the species name\n",
    "        else:\n",
    "          qual['host'] = brk_results[0]\n",
    "      if qual.get('host') == None:\n",
    "        qual['host'] = np.nan\n",
    "      elif qual.get('host') == '':\n",
    "        qual['host'] = np.nan\n",
    "      elif qual.get('host') and qual.get('host') != np.nan:\n",
    "        qual['host'] = qual['host'].strip().capitalize()\n",
    "        if qual['host'] == 'Sea coral':\n",
    "          qual['host'] = 'Coral'\n",
    "\n",
    "      # As a safe guard, check that the sequence is not empty\n",
    "      if record.seq:\n",
    "        sequence: Seq = record.seq\n",
    "        qual[\"sequence_length\"] = len(sequence)\n",
    "        save_records.append(record)\n",
    "\n",
    "      qualifiers.append(qual)\n",
    "\n",
    "  # Load features into a dataframe\n",
    "  df1 = DataFrame(qualifiers)\n",
    "  # Clean the table by replacing \"None\" and \"nan\"\n",
    "  df1.replace(\"None\", value=np.nan, inplace=True)\n",
    "  df1.replace(np.NaN, value=np.nan, inplace=True)\n",
    "  df1.replace(\"nan\", value=np.nan, regex=True, inplace=True)\n",
    "\n",
    "  # If not in debug mode, clean the table + save the files\n",
    "  if debug:\n",
    "    logger.debug(df1.head())\n",
    "    return df1, save_records\n",
    "  else:\n",
    "    df2 = clean_table(df1, granularity)\n",
    "    # Filter the records by the ids in the dataframe\n",
    "    filtered_recs = list(filter((lambda x: x.id in df2['id'].to_list()), save_records))\n",
    "\n",
    "    logger.info(f\"Records to save: {len(filtered_recs)}/{len(save_records)}\")\n",
    "\n",
    "    # Save the records to a file\n",
    "    save_file(f\"{rec_type}_seq.gb\", filtered_recs, \"genbank\")\n",
    "    save_file(f\"{rec_type}_seq.fasta\", filtered_recs, \"fasta\")\n",
    "\n",
    "    return df2, filtered_recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI: Popset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in NCBI (Popset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted by the NCBI's website, their `Popset database` is a collection of related sequences that is sourced from a single population/phylogenetic/mutation/ecosystem study. \n",
    "\n",
    "This is a useful grouping, as we would be able to extract the locational data from the metadata and use it to create a map of the distribution of the bacteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_popset(additional_ids: list[str] = []):\n",
    "  \"\"\"\n",
    "  Search for the popset database for the coral and Endozoicomonas.\n",
    "  :return: The number of results and the list of ids.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    term = f\"((Endozoicomonas[Organism] OR Endozoicomonas[All Fields]) AND coral[All Fields]\"\n",
    "    logger.debug(term)\n",
    "    handle = Entrez.esearch(db=\"popset\",\n",
    "      term=term,\n",
    "      retmax=20,\n",
    "    )\n",
    "    res = Entrez.read(handle)\n",
    "    if isinstance(res, DictionaryElement):\n",
    "      ids: list[str] = res[\"IdList\"]\n",
    "      ids.extend(additional_ids)\n",
    "      handle = Entrez.efetch(db=\"popset\", id=ids, rettype=\"gb\")\n",
    "      records: list[SeqRecord] = list(SeqIO.parse(handle, \"gb\"))\n",
    "      return len(records), ids, records\n",
    "    else:\n",
    "      return 0, [], []\n",
    "  except HTTPError as http_e:\n",
    "    logger.error(str(http_e))\n",
    "    return 0, [], []\n",
    "  except Exception as e:\n",
    "    logger.error(e)\n",
    "    return 0, [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After review of literature that happened in the inspection by Nucleotide DB, I found some popset that were worth adding manually.\n",
    "\n",
    "This includes the popset coming out of the work of {cite:p}`Speck_2012` (`\"227461503\"`), and the work of {cite:p}`Bayer_2013` (`\"510829312\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_ids_pop = [\n",
    "  '227461503',\n",
    "  '510829312',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_NEW:\n",
    "  count_pop, idlist_pop, records_pop = search_popset(additional_ids_pop)\n",
    "else:\n",
    "\n",
    "\n",
    "  records_pop = list(SeqIO.parse(os.path.join(dir_path, \"popset_seq.gb\"), \"gb\"))\n",
    "  count_pop = len(records_pop)\n",
    "  idlist_pop = [record.id for record in records_pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of popset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata table created for the data out of Popset database was saved to [features_of_popset_seq.csv](../../datasets/full_analysis/features_of_popset_seq.csv).\n",
    "\n",
    "Genbank file was saved as [popset_seq.gb](../../datasets/full_analysis/popset_seq.gb).\n",
    "Fasta file was saved as [popset_seq.fasta](../../datasets/full_analysis/popset_seq.fasta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:Found 2640 popset, sequences: 2640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Records to save: 2640/2640\n",
      "DEBUG:download_16s:genbank 2640 Saved\n",
      "DEBUG:download_16s:fasta 2640 Saved\n"
     ]
    }
   ],
   "source": [
    "# Load the metadata, and save the records to a file\n",
    "feat_df_pop0, recs_pop = table_features(count_pop, records_pop, \"popset\", granularity=GRANULARITY, debug=False)\n",
    "\n",
    "feat_df_pop0.to_csv(os.path.join(dir_path, \"features_of_popset_seq.csv\"), index=False)\n",
    "\n",
    "feat_df_pop = feat_df_pop0.copy()\n",
    "col_interest = ['id', 'sequence_name', 'group', 'organism', 'strain', 'host', 'country', 'taxon', 'BioProject', 'pubmed_ids', 'isolation_source', 'product']\n",
    "feat_df_pop = feat_df_pop[col_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country       isolation_source          host                  \n",
       "Bahamas       Tuna Alley (inside reef)  Plexaura sp.                1\n",
       "France        NaN                       Corallium rubrum            1\n",
       "Malaysia      NaN                       Coral reef ecosystems       5\n",
       "Saudi Arabia  Southern Red Sea          Acropora humilis          322\n",
       "                                        Pocillopora damicornis    496\n",
       "                                        Stylophora pistillata     354\n",
       "Taiwan        Green Island              Stylophora pistillata      17\n",
       "              Kenting                   Stylophora pistillata      21\n",
       "              Yeh Liu                   Stylophora pistillata       1\n",
       "USA           Hillsboro Ledge           Eunicea fusca               1\n",
       "              Summerland Key, Florida   Plexaura homomalla         25\n",
       "              West Maui                 Porites compressa         758\n",
       "NaN           NaN                       Coral                      58\n",
       "                                        Coral mucus                 4\n",
       "                                        Eunicella cavolini         20\n",
       "                                        Stylophora pistillata     556\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df_pop.groupby(['country', 'isolation_source','host'], dropna=False)['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI: Nucleotide databse\n",
    "\n",
    "While **+1500** results is absolute enough to work with, I will expand the scope to include more sequences by searching directly through the nucleotide database.\n",
    "\n",
    "I will combine the popset dataset and the findings below, and then remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in NCBI (Nucleotide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review of literature revealed one nucleotide sequence that was not included by the search using `search_nucleo(...)` function, which found a match to a coral species found in the Red Sea {cite}`Pogoreutz2022`. We will manually note this, and add it to our list of sequences to download. \n",
    "\n",
    "\n",
    "Interestingly, looking more into the referenced sequence revealed that the original study had a total of 412 16S Endozoicomonas sequences to work with {cite}`Bayer_2013`. This will be added by adding the accession numbers by Popset (above).\n",
    "\n",
    "Additionally, they {cite}`Bayer_2013` also provided a supplementary phylogenetic tree. We will use this to add more accession ids to our list of sequences to download.\n",
    "\n",
    "```{figure} ../../outputs/full_analysis/bayer_2013_fig_s3_1pg_rotated.png\n",
    "---\n",
    "name: parsimony-tree-bayer-2013\n",
    "---\n",
    "Tree showing the phylogenetic relationship of Endozoicomonas to other bacterial species.\n",
    "```\n",
    "\n",
    "These are the accession IDs mentioned in the tree above.\n",
    "```{csv-table}\n",
    "   :file: ../../datasets/full_analysis/extra_accessions.csv\n",
    "   :header-rows: 1\n",
    "```\n",
    "\n",
    "> `DQ884169  (C19)` `DQ884170 (C23)`, `DQ884160 (CRNA5)` noted with \"colony sequences C19, C23 and CRNA5 [...] formed a group related to Endozoiciomonas elysicola (97.6–99.7% similarity), a bacterium isolated from the marine mollusc Elysia ornata (M. Kurahashi, unpublished) {cite}`MartinezGarcia2006`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nucleo(limit: int, additional_ids: list[str] = []):\n",
    "  \"\"\"\n",
    "  Search for the nucleotide database for nucleotides that mention coral and Endozoicomonas.\n",
    "\n",
    "  :param `limit`: The maximum number of results to return.\n",
    "  \"\"\"\n",
    "\n",
    "  term = \"(coral[All Fields] AND 16S[Title]) AND Endozoicomonas[Organism]\"\n",
    "  try:\n",
    "    handle = Entrez.esearch(db=\"nucleotide\",\n",
    "      term=term,\n",
    "      retmax=limit,\n",
    "    )\n",
    "    res = Entrez.read(handle)\n",
    "    if isinstance(res, DictionaryElement):\n",
    "      ids: list[str] = res[\"IdList\"]\n",
    "      ids.extend(additional_ids)\n",
    "      logger.debug(ids)\n",
    "      handle = Entrez.efetch(db=\"nucleotide\", id=ids, rettype=\"gb\")\n",
    "      records: list[SeqRecord] = list(SeqIO.parse(handle, \"gb\"))\n",
    "      return len(records), ids, records\n",
    "    else:\n",
    "      return 0, [], []\n",
    "  except HTTPError as http_e:\n",
    "    logger.error(http_e.read())\n",
    "    logger.exception(http_e)\n",
    "    return 0, [], []\n",
    "  except Exception as e:\n",
    "    logger.error(e)\n",
    "    logger.exception(e)\n",
    "    return 0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:['2178380722', '2178380718', '2168685149', '2168685148', '2168685147', '2168685146', '2168685145', '2168685144', '2168685143', '2168685142', '2168685141', '2168685140', '2168685139', '2168685138', '2168685137', '2168685136', '2168685135', '2168685134', '2168685133', '2168685132', '2168685131', '2168685130', '2168685129', '2168685128', '2168685127', '2168685126', '2168685125', '2168685124', '2168685123', '2168685122', '2168685121', '2168685120', '2168685119', '2168685118', '2168685117', '2168685116', '2168685115', '2168685114', '2168685113', '2168685112', '2168685111', '2168685110', '2168685109', '2168685108', '2168685107', '2168685106', '2168685105', '2168685104', '2168685103', '2168685102', '2168685101', '2168685100', '2168685099', '2168685098', '2168685097', '2168685096', '2168685095', '2168685094', '2168685093', '2168685092', '2168685091', '2168685090', '2168685089', '2168685088', '2168685087', '2168685086', '2168685085', '2168685084', '2168685083', '1967466870', '1967466869', '1864207121', '590924463', '406364834', '1445136087', '636560549', '1339457653', '1339457639', '1339457630', '1339457625', '1339457623', '1039091095', '928189784', '916534561', '724785349', '724785345', '724785336', '530788500', '530788496', '530788490', '530788477', '530788476', '530788474', '530788473', '530788471', '530788465', '530788457', '530788456', '530788451', '530788450', '530788449', '530788448', '530788447', '530788445', '530788444', '530788440', '530788437', '530788430', '530788429', '530788420', '530788419', '530788418', '359391828', '348161577', '348161576', '348161575', '348161574', '348161573', '348161572', '348161571', '348161570', '348161569', '348161568', '348161567', '348161566', '348161565', '348161564', '348161563', '348161562', '348161561', '348161560', '348161559', '348161558', '348161557', '348161556', '348161555', '348161554', '348161553', '348161552', '348161551', '348161550', '348161549', '348161548', '348161547', '348161546', '348161545', '348161544', '348161543', '348161542', '348161541', '348161540', '304368101', '210160953', 'GU118644', 'GU118168', 'GU118379', 'GU118072', 'GU118404', 'GU118957', 'GU784983', 'GU118966', 'AB695088', 'AM259915', 'DQ884169', 'DQ884170', 'DQ884160', 'AY700600', 'AY700601', 'FJ202634', 'FJ347758', 'FJ930289', 'AB196667', 'FM162182', 'AB695089', 'KX611231', 'HE818343', 'KY798517', 'FM162188.1', 'FM244838.1', 'FM162184.1', 'FM162185.1', 'EU797579.3', 'GU569894.1', 'GU183820.1', 'KJ372479.1', 'AB695088', 'AB196667']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "more_ids = read_csv(os.path.realpath(\"../../datasets/full_analysis/extra_accessions.csv\"), header=0)\n",
    "remove_ids_df = read_csv(os.path.realpath(\"../../datasets/full_analysis/excluded_accessions.csv\"))\n",
    "remove_ids = remove_ids_df['Accession ID'].to_list()\n",
    "\n",
    "additional_accessions = more_ids['Accession ID'].to_list()\n",
    "SEARCH_NEW = True\n",
    "if SEARCH_NEW:\n",
    "  count_nuc, idlist_nuc, records_nuc = search_nucleo(4000, additional_accessions)\n",
    "else:\n",
    "  records_nuc: list[SeqRecord] = list(SeqIO.parse(os.path.join(dir_path, \"nucleotide_seq.gb\"), \"gb\"))\n",
    "\n",
    "records_nuc = list(filter(lambda x: x.id not in remove_ids, records_nuc))\n",
    "count_nuc = len(records_nuc)\n",
    "idlist_nuc = [record.id for record in records_pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of nucleotide database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the...\n",
    "| Type of data | File name |\n",
    "|--------------|-----------|\n",
    "| Metadata | [features_of_nucleotide_seq.csv](../../datasets/full_analysis/features_of_nucleotide_seq.csv) |\n",
    "| Genbank | [nucleotide_seq.gb](../../datasets/full_analysis/nucleotide_seq.gb) |\n",
    "| Fasta | [nucleotide_seq.fasta](../../datasets/full_analysis/nucleotide_seq.fasta) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything combined (nucleotide + popset)\n",
    "\n",
    "We will save the...\n",
    "| Type of data | File name |\n",
    "|--------------|-----------|\n",
    "| Metadata | [feat_all_seq.csv](../../datasets/full_analysis/feat_all_seq.csv) |\n",
    "| Genbank | [all_seq.gb](../../datasets/full_analysis/all_seq.gb) |\n",
    "| Fasta | [all_seq.fasta](../../datasets/full_analysis/all_seq.fasta) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_latlng():\n",
    "  latlng_file = read_excel(os.path.join(dir_path, \"../../datasets/full_analysis/handwritten_latlng.xlsx\"), sheet_name=\"Sheet1\")\n",
    "  latlng_file['country'] = latlng_file['country'].astype(str)\n",
    "  # switch every \"nan\" to np.nan\n",
    "  latlng_file.replace(\"nan\", value=np.nan, inplace=True)\n",
    "  latlng_file[['lat','long']] = latlng_file['latlng'].str.split(',',expand=True)\n",
    "  latlng_file['lat'].str.strip()\n",
    "  latlng_file['long'].str.strip()\n",
    "  latlng_file['lat_dec'] = latlng_file.apply(lambda row: parse(row['lat']), axis=1)\n",
    "  latlng_file['long_dec'] = latlng_file.apply(lambda row: parse(row['long']), axis=1)\n",
    "  return latlng_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel file\n",
    "latlng_file = load_latlng()\n",
    "\n",
    "def annotate_groups(df0: DataFrame):\n",
    "  df = df0.copy()\n",
    "  # First, saudi arabia\n",
    "  # Fix group KC669\n",
    "\n",
    "  is_kc669_668 = df['id'].str.contains('KC669') | df['id'].str.contains('KC668')\n",
    "  df.loc[is_kc669_668, 'country'] = 'Saudi Arabia'\n",
    "  is_acropora_h = (df['country'] == 'Saudi Arabia') & (df['host'] == 'Acropora humilis')\n",
    "  is_pocillopora_d = (df['country'] == 'Saudi Arabia') & (df['host'] == 'Pocillopora damicornis')\n",
    "\n",
    "  df.loc[is_acropora_h, 'latlng'] = \"19°52'24.48'N, 40°04'46.14'E\"\n",
    "  df.loc[is_acropora_h, 'isolation_source'] = \"Red Sea, Brown reef\"\n",
    "\n",
    "\n",
    "  df.loc[is_pocillopora_d, 'latlng'] = \"19°52'24.48'N, 40°04'46.14'E\"\n",
    "  df.loc[is_pocillopora_d, 'isolation_source'] = \"Red Sea, Brown reef\"\n",
    "  df.loc[is_pocillopora_d | is_acropora_h, 'collection_date'] = date(2010, 7, 1)\n",
    "\n",
    "  is_stylopora = df['host'] == 'Stylophora pistillata'\n",
    "\n",
    "  is_site_5 = df['clone'].str.contains('05')\n",
    "  is_site_12 = df['clone'].str.contains('12')\n",
    "  is_site_14 = df['clone'].str.contains('14')\n",
    "  is_site_15 = df['clone'].str.contains('15')\n",
    "  is_site_17 = df['clone'].str.contains('17')\n",
    "\n",
    "  df.loc[is_kc669_668 | is_stylopora, 'collection_date'] = date(2009, 6, 1)\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_5, 'latlng'] = \"18°40'30.36'N, 40°44'21.18'E\"\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_12, 'latlng'] = \"18°40'30.36'N, 40°44'21.18'E\"\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_14, 'latlng'] = \"19°53'52.74'N, 40° 0'53.46'E\"\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_15, 'latlng'] = \"19°53'15.42'N, 40°9'23.94'E\"\n",
    "  df.loc[is_kc669_668 & is_stylopora & is_site_17, 'latlng'] = \"20°8'58.38'N, 40°14'7.50'E\"\n",
    "  df.loc[is_kc669_668 & is_stylopora, 'isolation_source'] = \"Southern Red Sea\"\n",
    "  df.replace(r\"nan\",np.nan, regex=True, inplace=True)\n",
    "  # set lat and long by splitting the latlng column in saudi_group\n",
    "  latlng_saudi = df.loc[df['country'] == 'Saudi Arabia']['latlng'].str.split(',', n=2, expand=True)\n",
    "  df.loc[(df['country'] == 'Saudi Arabia'), 'lat'] = latlng_saudi[0]\n",
    "  df.loc[(df['country'] == 'Saudi Arabia'), 'long'] = latlng_saudi[1]\n",
    "  df.loc[(df['country'] == 'Saudi Arabia'), 'lat_dec'] = df.loc[(df['country'] == 'Saudi Arabia')].apply(lambda row: parse(row['lat']), axis=1)\n",
    "  df.loc[(df['country'] == 'Saudi Arabia'), 'long_dec'] = df.loc[(df['country'] == 'Saudi Arabia')].apply(lambda row: parse(row['long']), axis=1)\n",
    "\n",
    "  # remove extra maui\n",
    "  is_Maui = df['isolation_source'] == 'West Maui'\n",
    "  has_c7 = df['clone'] == 'C7-A01c'\n",
    "  keep_df = df.loc[is_Maui & has_c7].copy()\n",
    "  df = df.loc[~is_Maui]\n",
    "  df = concat([df, keep_df])\n",
    "\n",
    "  # deal with australia\n",
    "  df.loc[df['id'].str.contains('KM3604'), 'country'] = 'Australia'\n",
    "  df.loc[df['id'].str.contains('KM3604'), 'isolation_source'] = 'Orpheus Island Great Barrier Reef'\n",
    "\n",
    "  # deal with taiwan\n",
    "  is_taiwan_group = df['id'].str.contains('JN635')\n",
    "  is_green_island = df['isolation_source'] == 'Green Island'\n",
    "  # Split the clone column\n",
    "  df['clone_num'] = df.loc[is_taiwan_group & is_green_island]['clone'].str.split('U', expand=True)[1].astype(int)\n",
    "  # if the clone num is less than 1000, it's a Kunguan group\n",
    "  is_kunguan_group = df['clone_num'] < 1000\n",
    "  # otherwise it's a Chaikou, Green Island\n",
    "  is_chaikou_group = df['clone_num'] >= 1000\n",
    "  df.loc[is_taiwan_group & is_green_island & is_kunguan_group, 'isolation_source'] = \"Kunguan, Green Island\"\n",
    "  df.loc[is_taiwan_group & is_green_island & is_chaikou_group, 'isolation_source'] = \"Chaikou, Green Island\"\n",
    "\n",
    "  # Remove some of the extra taiwan\n",
    "  to_remove = ['NR_116609.1', 'NR_158127.1', 'NR_169415.1']\n",
    "  df = df.loc[~df['id'].isin(to_remove)]\n",
    "\n",
    "  # if the host is any of \"Coral\", or \"Coral mucus\" or \"Marine sponge\" or \"Coral reef ecosystems\" set to nan\n",
    "  df.loc[df['host'].isin(['Coral', 'Coral mucus', 'Marine sponge', 'Coral reef ecosystems']), 'host'] = np.nan\n",
    "\n",
    "   # replace \"gut\" in isolation_source with np.nan\n",
    "  df['isolation_source'].replace('gut', np.nan, inplace=True)\n",
    "\n",
    "  # set isolation_source to np.nan where id = \"FJ202634\"\n",
    "  df.loc[df['id'] == 'FJ202634.1', 'isolation_source'] = np.nan\n",
    "\n",
    "  for index, row in latlng_file.iterrows():\n",
    "    group = row['group']\n",
    "\n",
    "    id_is_group = df['id'].str.contains(group)\n",
    "    df.loc[id_is_group, 'doi'] = row['doi']\n",
    "\n",
    "\n",
    "    if isinstance(row['country'], str):\n",
    "      df.loc[id_is_group & (df['country'].isna()), 'country'] = row['country']\n",
    "\n",
    "    if isinstance(row['host'], str):\n",
    "      df.loc[id_is_group & (df['host'].isna()), 'host'] = row['host']\n",
    "\n",
    "    if group != 'JN635':\n",
    "      df.loc[id_is_group, 'collection_date'] = row['date']\n",
    "      df.loc[id_is_group, 'latlng'] = row['latlng']\n",
    "      df.loc[id_is_group, 'lat_dec'] = row['lat_dec']\n",
    "      df.loc[id_is_group, 'long_dec'] = row['long_dec']\n",
    "\n",
    "      if isinstance(row['source'], str) and row['source'] != '':\n",
    "        df.loc[id_is_group & (df['isolation_source'].isna()), 'isolation_source'] = row['source']\n",
    "\n",
    "    else:\n",
    "      find_matching_taiwan = id_is_group & (df['isolation_source'] == row['source'])\n",
    "      df.loc[find_matching_taiwan, 'collection_date'] = row['date']\n",
    "      df.loc[find_matching_taiwan, 'latlng'] = row['latlng']\n",
    "\n",
    "      df.loc[id_is_group, 'lat_dec'] = row['lat_dec']\n",
    "      df.loc[id_is_group, 'long_dec'] = row['long_dec']\n",
    "\n",
    "  df['collection_date'] = to_datetime(df['collection_date'])\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:Found 187 nucleotide, sequences: 187\n",
      "INFO:download_16s:Records to save: 187/187\n",
      "DEBUG:download_16s:genbank 187 Saved\n",
      "DEBUG:download_16s:fasta 187 Saved\n"
     ]
    }
   ],
   "source": [
    "feat_df_nuc0, recs_nuc = table_features(count_nuc, records_nuc, \"nucleotide\", granularity=GRANULARITY, debug=False)\n",
    "feat_df_nuc0.to_csv(os.path.join(dir_path, \"features_of_nucleotide_seq.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Records found: 1883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Records found: 1123\n"
     ]
    }
   ],
   "source": [
    "feat_df_all = concat([feat_df_nuc0, feat_df_pop0]).drop_duplicates(subset=\"id\")\n",
    "# print rows with country = Brazil\n",
    "\n",
    "\n",
    "logger.info(f\"Records found: {len(feat_df_all['id'].to_list())}\")\n",
    "feat_df_all = annotate_groups(feat_df_all)\n",
    "\n",
    "logger.info(f\"Records found: {len(feat_df_all['id'].to_list())}\")\n",
    "\n",
    "col_interest = ['id', 'country', 'group', 'host', 'isolation_source', 'sequence_length', 'sequence_name', 'collection_date', 'latlng', 'lat_dec', 'long_dec']\n",
    "# ['id', 'clone', 'sequence_name', 'group', 'organism', 'host', 'country', 'pubmed_ids', 'isolation_source','sequence_length']\n",
    "feat_df_all = feat_df_all[col_interest].sort_values(by=['sequence_length'], ascending=False).sort_values(by=['id']).reset_index(drop=True)\n",
    "feat_df_all.to_csv(os.path.join(dir_path, \"feat_all_seq.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>group</th>\n",
       "      <th>host</th>\n",
       "      <th>isolation_source</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>latlng</th>\n",
       "      <th>lat_dec</th>\n",
       "      <th>long_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, country, group, host, isolation_source, sequence_length, sequence_name, collection_date, latlng, lat_dec, long_dec]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find nan collection_date\n",
    "feat_df_all.loc[feat_df_all['collection_date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>group</th>\n",
       "      <th>host</th>\n",
       "      <th>isolation_source</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>latlng</th>\n",
       "      <th>lat_dec</th>\n",
       "      <th>long_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, country, group, host, isolation_source, sequence_length, sequence_name, collection_date, latlng, lat_dec, long_dec]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find nan lat_dec\n",
    "feat_df_all.loc[feat_df_all['lat_dec'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>group</th>\n",
       "      <th>host</th>\n",
       "      <th>isolation_source</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>latlng</th>\n",
       "      <th>lat_dec</th>\n",
       "      <th>long_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, country, group, host, isolation_source, sequence_length, sequence_name, collection_date, latlng, lat_dec, long_dec]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find nan long_dec\n",
    "feat_df_all.loc[feat_df_all['long_dec'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:download_16s:Unique records found: 1123 / 1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:download_16s:genbank 1123 Saved\n",
      "DEBUG:download_16s:fasta 1123 Saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'country', 'group', 'host', 'isolation_source', 'sequence_length',\n",
      "       'sequence_name', 'collection_date', 'latlng', 'lat_dec', 'long_dec'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "unique_recs = unique([recs_nuc, recs_pop], feat_df_all)\n",
    "logger.info(f\"Unique records found: {len(unique_recs)} / {len(feat_df_all['id'].to_list())}\")\n",
    "# Save the records to a file\n",
    "save_file(\"all_seq.gb\", unique_recs, \"genbank\")\n",
    "save_file(\"all_seq.fasta\", unique_recs, \"fasta\")\n",
    "print(feat_df_all.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic details\n",
    "\n",
    "Here shows the breakdown by country, and by isolation source as reported by the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_source = feat_df_all.groupby(['country', 'isolation_source'], dropna=False).head(2)\n",
    "country_source[['country', 'isolation_source', 'id']].to_csv(os.path.join(dir_path, \"country_source.csv\"), index=True)\n",
    "country_source['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country         group  host                               isolation_source                                                    \n",
       "Atlantic Ocean  FM162  Bathymodiolus puteoserpentis       Mid-Atlantic Ridge, Logatchev hydrothermal vent field, IRINA II site      1\n",
       "Australia       AY700  Pocillopora damicornis             Great Barrier Reef                                                        2\n",
       "                GU784  Ianthella basta                    Masig Island, central Torres Strait                                       1\n",
       "                KM360  Lobophytum compactum               Orpheus Island Great Barrier Reef                                         3\n",
       "Bahamas         JX488  Plexaura sp.                       Tuna Alley (inside reef)                                                  1\n",
       "Brazil          GU183  Mucus of apparently healthy coral  Preta Beach, Sao Sebastiao Channel, Sao Paulo                             1\n",
       "                KJ372  Arenosclera brasiliensis           João Fernandinho Beach, Búzios, Rio de Janeiro                            1\n",
       "Croatia         AM259  Sponge mesohyl                     Adriatic Sea                                                              1\n",
       "France          JQ691  Eunicella cavolini                 Bay of Villefranche-sur-Mer                                              20\n",
       "                KT964  Corallium rubrum                   Porticcio, Corsica Island                                                 1\n",
       "Germany         KX611  Sponge haliclona sp.               Justus-Liebig-University Giessen                                          1\n",
       "                KY798  Sponge                             Sponge fragment                                                           1\n",
       "Japan           AB196  Elysia ornata                      coast of Izu-Miyake Island                                                1\n",
       "                AB695  Haplosclerida gen. et sp.          Shizuoka, Numazu                                                          2\n",
       "                OL957  Stylophora pistillata              Okinawa                                                                  39\n",
       "Kuwait          HM804  Acropora clathrata                 Qit'at Bnaider                                                            1\n",
       "Malaysia        MG896  Clavelina coerulea                 Bidong Island, Terengganu                                                 5\n",
       "Mexico          FM162  Bathymodiolus brooksi              Gulf of Mexico, Campeche Knolls asphalt volcano, Chapopote site           1\n",
       "Panama          FJ202  Montastraea faveolata              Isla San Cristobal, Bocas del Toro                                        1\n",
       "                GU118  Acropora palmata                   “Crawl Cay” reef near Bocas del Toro                                      1\n",
       "                       Diploria strigosa                  “Crawl Cay” reef near Bocas del Toro                                      1\n",
       "                       Gorgonia ventalina                 “Crawl Cay” reef near Bocas del Toro                                      2\n",
       "                       Montastraea faveolata              “Crawl Cay” reef near Bocas del Toro                                      1\n",
       "                       Porites astreoides                 “Crawl Cay” reef near Bocas del Toro                                      2\n",
       "Portugal        HE818  Irciniidae sp.                     Algarve, Gale Alta, Armacao de Pera                                       1\n",
       "Saudi Arabia    KC668  Acropora humilis                   Red Sea, Brown reef                                                     161\n",
       "                       Pocillopora damicornis             Red Sea, Brown reef                                                     248\n",
       "                       Stylophora pistillata              Southern Red Sea                                                        177\n",
       "                KC669  Stylophora pistillata              Southern Red Sea                                                        278\n",
       "                MG725  P. verrucosa                       Al-Fahal reef                                                            58\n",
       "                MW540  Acropora cytherea                  Red Sea                                                                   1\n",
       "                       Acropora hemprichii                Red Sea                                                                   1\n",
       "Spain           DQ884  Cystodytes dellechiajei            Mediterranean Sea                                                         3\n",
       "Taiwan          FJ347  Montipora aequituberculata         Kenting                                                                   1\n",
       "                GU569  Stylophora pistillata              ocean water                                                               1\n",
       "                JN635  Stylophora pistillata              Chaikou, Green Island                                                    10\n",
       "                                                          Kenting                                                                  21\n",
       "                                                          Kunguan, Green Island                                                     7\n",
       "                JN848  Stylophora pistillata              Yeh Liu                                                                   1\n",
       "                LN875  Acropora  sp.                      Kenting                                                                   1\n",
       "                LN879  Acropora  sp.                      Kenting                                                                   1\n",
       "                OL957  Stylophora pistillata              Kenting                                                                  28\n",
       "USA             EU797  Alitta succinea                    Grice Cove, South Carolina                                                1\n",
       "                FJ930  Porites compressa                  West Maui                                                                 1\n",
       "                FM162  Bathymodiolus brooksi              Gulf of Mexico, Atwater Valley cold seep                                  1\n",
       "                       Bathymodiolus heckerae             Gulf of Mexico, West Florida Escarpment cold seep                         1\n",
       "                FM244  Bathymodiolus childressi           Gulf of Mexico, Mississippi Canyon cold seep                              1\n",
       "                JX488  Eunicea fusca                      Hillsboro Ledge                                                           1\n",
       "                KF428  Plexaura homomalla                 Summerland Key, Florida                                                  25\n",
       "                OM273  Eunicea flexuosa                   Florida Keys                                                              2\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouping = ['country', 'group', 'host', 'isolation_source']\n",
    "feat_df_all.groupby(grouping, dropna=False)['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must load the outgroup sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from other_taxa_16S\\\\16S_other_taxa_all.fasta\n",
    "# list files that end with .fna in  other_taxa_16S\n",
    "list_fna = [f for f in os.listdir(os.path.join(dir_path, \"other_taxa_16S\")) if f.endswith('.fna')]\n",
    "# load the fasta files\n",
    "other_taxa : list[SeqRecord] = []\n",
    "for fna in list_fna:\n",
    "  other_taxa.extend(list(SeqIO.parse(os.path.join(dir_path, \"other_taxa_16S\", fna), \"fasta\")))\n",
    "\n",
    "# load the metadata from other_taxa_16S\\\\other_taxa.csv\n",
    "other_taxa_df = read_csv(os.path.join(dir_path, \"other_taxa_16S\\\\other_taxa.csv\"))\n",
    "# split latlng into lat and long\n",
    "other_taxa_df[['lat','long']] = other_taxa_df['latlng'].str.split(' ',expand=True)\n",
    "# read collection_date as date\n",
    "other_taxa_df['collection_date'] = to_datetime(other_taxa_df['collection_date'])\n",
    "# parse lat and long into decimal\n",
    "other_taxa_df['lat_dec'] = other_taxa_df.apply(lambda row: parse(row['lat']), axis=1)\n",
    "other_taxa_df['long_dec'] = other_taxa_df.apply(lambda row: parse(row['long']), axis=1)\n",
    "\n",
    "other_taxa_df = other_taxa_df.drop(columns=['lat', 'long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>latlng</th>\n",
       "      <th>lat_dec</th>\n",
       "      <th>long_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NZ_JAEVHF010000042.1:129-1670</td>\n",
       "      <td>10.1099/ijsem.0.005055</td>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>37°24'06.0\"N 130°13'36.4\"E</td>\n",
       "      <td>37.401667</td>\n",
       "      <td>130.226778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZ_PZJW01000013.1:348-1887</td>\n",
       "      <td>10.1099/ijsem.0.003061</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>40°39'32\"N 8°48'40\"W</td>\n",
       "      <td>40.658889</td>\n",
       "      <td>-8.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NZ_FWPT01000026.1:38-1347</td>\n",
       "      <td>10.1016/j.syapm.2017.11.004</td>\n",
       "      <td>2017-03-24</td>\n",
       "      <td>50°34'08\"N 8°40'22\"E</td>\n",
       "      <td>50.568889</td>\n",
       "      <td>8.672778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NZ_SMME01000314.1:1-536</td>\n",
       "      <td>10.1099/ijsem.0.002781</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>24.0776°N 74.476°W</td>\n",
       "      <td>24.077600</td>\n",
       "      <td>-74.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NZ_QWBW01000001.1:22479-24016</td>\n",
       "      <td>10.1007/s00284-019-01674-z</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>17°57'48\"S 38°41'30\"Z</td>\n",
       "      <td>-17.963333</td>\n",
       "      <td>38.691667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id                          doi collection_date  \\\n",
       "0  NZ_JAEVHF010000042.1:129-1670       10.1099/ijsem.0.005055      2007-04-01   \n",
       "1     NZ_PZJW01000013.1:348-1887       10.1099/ijsem.0.003061      2010-11-11   \n",
       "2      NZ_FWPT01000026.1:38-1347  10.1016/j.syapm.2017.11.004      2017-03-24   \n",
       "3        NZ_SMME01000314.1:1-536       10.1099/ijsem.0.002781      2018-07-09   \n",
       "4  NZ_QWBW01000001.1:22479-24016   10.1007/s00284-019-01674-z      2015-07-01   \n",
       "\n",
       "                       latlng    lat_dec    long_dec  \n",
       "0  37°24'06.0\"N 130°13'36.4\"E  37.401667  130.226778  \n",
       "1        40°39'32\"N 8°48'40\"W  40.658889   -8.811111  \n",
       "2        50°34'08\"N 8°40'22\"E  50.568889    8.672778  \n",
       "3          24.0776°N 74.476°W  24.077600  -74.476000  \n",
       "4       17°57'48\"S 38°41'30\"Z -17.963333   38.691667  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_taxa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tip_dates(df0: DataFrame, file_name: str):\n",
    "  df = df0.drop(['latlng'], axis=1)\n",
    "  df['collection_date_str'] = df['collection_date'].dt.strftime('%Y-%m-%d')\n",
    "  df[['id', 'collection_date_str']].to_csv(os.path.join(dir_path, \"../../outputs/full_analysis\", file_name), index=False, sep=\"\\t\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trait_values(df0: DataFrame, file_name: str):\n",
    "  df = df0.drop(['latlng'], axis=1)\n",
    "  df['collection_date_str'] = df['collection_date'].dt.strftime('%Y-%m-%d').astype(str)\n",
    "  df.rename(columns={'lat_dec': 'lat', 'long_dec': 'long'}, inplace=True)\n",
    "\n",
    "  print(len(df))\n",
    "  df[['id', 'lat', 'long']].to_csv(os.path.join(dir_path, \"../../outputs/full_analysis\", file_name), index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "to_align_df = feat_df_all.groupby(grouping, dropna=False).head(5)\n",
    "\n",
    "to_align = [rec for rec in unique_recs if rec.id in to_align_df['id'].to_list()]\n",
    "# concatenate other_taxa and to_align\n",
    "all_with_other = to_align + other_taxa\n",
    "\n",
    "all_with_other_df = concat([to_align_df, other_taxa_df])\n",
    "print(len(all_with_other_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "save_trait_values(all_with_other_df, f\"{len(all_with_other_df)}_trait_values.txt\")\n",
    "save_tip_dates(all_with_other_df, f\"{len(all_with_other_df)}_tip_dates.txt\")\n",
    "# save_file(f\"{len(all_with_other)}_to_align.fasta\", all_with_other, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align the 16S sequences\n",
    "\n",
    "Use muscle to align the sequences.\n",
    "\n",
    "```bash\n",
    "muscle5.1 -align ./datasets/full_analysis/117_to_align.fasta -output ./outputs/full_analysis/muscle/aligned_top_117.fasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the descrption in the fasta file to make using the file in BEAST easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment length 1713\n",
      "Number of records 96\n"
     ]
    }
   ],
   "source": [
    "# Read aligned fasta file\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "\n",
    "alignment = AlignIO.read(os.path.realpath(f\"..\\\\..\\\\outputs\\\\full_analysis\\\\muscle\\\\aligned_top_{len(all_with_other)}.fasta\"), \"fasta\")\n",
    "if isinstance(alignment, MultipleSeqAlignment):\n",
    "  print(f\"Alignment length {alignment.get_alignment_length()}\")\n",
    "  print(f\"Number of records {len(alignment)}\")\n",
    "  for seq in alignment:\n",
    "    seq.description = seq.id\n",
    "\n",
    "  # Save the alignment to a file\n",
    "  AlignIO.write(alignment, os.path.realpath(f\"../../outputs/full_analysis/muscle/{len(all_with_other)}_aligned_name_replaced.fasta\"), \"fasta\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
